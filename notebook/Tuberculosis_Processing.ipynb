{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install watson-developer-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions, SemanticRolesOptions\n",
    "from watson_developer_cloud import WatsonException \n",
    "\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk\n",
    "import json\n",
    "\n",
    "\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your credentials\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-03-23',\n",
    "    username=\"\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right-click data_doctor.zip, add it as a streaming body and name it as streaming_body_10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "feature_s=[]\n",
    "file_name='train.csv' # Name of csv output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractZipFilesAndGetContent():\n",
    "    'extracting data from the zip files'\n",
    "    zip_ref = zipfile.ZipFile(BytesIO(streaming_body_10.read()),'r')\n",
    "    paths = zip_ref.namelist()\n",
    "    req_path=dict()\n",
    "    temp=[]\n",
    "    check=''\n",
    "    content=''\n",
    "    factsheet=''\n",
    "    for path in paths:\n",
    "        factsheet=zip_ref.extract(path)\n",
    "        print(factsheet)\n",
    "        test=path.split('/')\n",
    "        print(test)\n",
    "        if paths.index(path) < len(paths)-2:\n",
    "            if len(test) == 4:\n",
    "                if not test[3]:\n",
    "                    temp.append(paths[paths.index(path)+1])\n",
    "                    if 'lab' in paths[paths.index(path)+2]:\n",
    "                        temp.append(paths[paths.index(path)+2])\n",
    "                    req_path[test[2]]= temp\n",
    "                    temp=[]\n",
    "    zip_ref.close() \n",
    "    pdf = PdfFileReader(open(factsheet,'rb'))\n",
    "    'Extract text from page.'\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        content += pdf.getPage(i).extractText() + \"\\n\"\n",
    "    content = \" \".join(content.replace(\"\\xa0\", \" \").strip().split())\n",
    "    return req_path,content\n",
    "\n",
    "\n",
    "\n",
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    \"\"\"\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(entities= EntitiesOptions(), keywords=KeywordsOptions(), semantic_roles=SemanticRolesOptions()))\n",
    "    return response\n",
    "\n",
    "def extractFeatures():\n",
    "    entity=[]\n",
    "    keyword=[]\n",
    "    semantic=[]\n",
    "    sample_txt=[]\n",
    "    subject=[]\n",
    "    objects=[]\n",
    "    attribute=[]\n",
    "\n",
    "    '''Put all the text that has symptoms or symptom keyword into sample_txt'''\n",
    "\n",
    "    for i in sent_text:\n",
    "        if \"symptoms\" in i:\n",
    "            sample_txt.append(i)\n",
    "        elif \"symptom\" in i:\n",
    "            sample_txt.append(i)\n",
    "\n",
    "    '''Extract entities,keywords,semantic rule'''\n",
    "\n",
    "    for x in sample_txt:\n",
    "        entity= entity+ analyze_using_NLU(x)['entities']\n",
    "        keyword= keyword+analyze_using_NLU(x)['keywords']\n",
    "        semantic= semantic+analyze_using_NLU(x)['semantic_roles']\n",
    "\n",
    "#     '''Find subject,object and action from semantic rules'''    \n",
    "#     for i in semantic:\n",
    "#         print(\"subject:\",i['subject']['text'])\n",
    "#         print(\"object:\",i['object']['text'])\n",
    "#         print(\"action:\",i['action']['text'])\n",
    "#         print(i['subject']['text'],i['action']['text'],i['object']['text'])\n",
    "#         print('-------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    '''Find object containing symptoms or symptom as text'''  \n",
    "\n",
    "    for i in semantic:\n",
    "        if i['object']['text'] == \"symptoms\" or i['object']['text'] == \"symptom\":\n",
    "            subject.append(i['subject']['text'])\n",
    "            objects.append(i['object']['text'])\n",
    "\n",
    "    for k in subject:\n",
    "        attribute.append(k.split(','))\n",
    "    '''This is the feature set'''\n",
    "\n",
    "    for i in attribute:\n",
    "        for j in i:\n",
    "            if 'and' in j:\n",
    "                j=j.replace('and','')\n",
    "                j=j.lstrip()\n",
    "                feature_s.append(j)\n",
    "            else:\n",
    "                j=j.lstrip()\n",
    "                feature_s.append(j)\n",
    "    return feature_s\n",
    "\n",
    "def lab_reports(path):\n",
    "    global tests\n",
    "    '''Feed your pdf file'''\n",
    "    pdf = PdfFileReader(open(path,'rb'))\n",
    "\n",
    "    '''Store the content in a variable'''\n",
    "    content=''\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        content += pdf.getPage(i).extractText() + \"\\n\"\n",
    "\n",
    "    '''Algorithm to extract key value pairs'''\n",
    "    sent_text = nltk.sent_tokenize(content) \n",
    "    req=sent_text[3]\n",
    "    req=req.split('Interpretation')[0]\n",
    "    req=req.splitlines()\n",
    "    req= [x.strip() for x in req]\n",
    "    req=list(filter(None,req))\n",
    "    matching=[]\n",
    "    for i in range(len(tests)):\n",
    "        matching= matching +([s for s in req if tests[i].lower() in s.lower()])\n",
    "    start_index=req.index(matching[0])\n",
    "    test=dict()\n",
    "    f=False\n",
    "    key=[]\n",
    "    values=[]\n",
    "    value=''\n",
    "    for i in range(start_index,len(req)):\n",
    "        if req[i] in matching:\n",
    "            if i!=start_index:\n",
    "                values.append(value)\n",
    "            value=''\n",
    "            key.append(req[i])\n",
    "            f=True\n",
    "        elif f and req[i] not in matching:\n",
    "            if value != '':\n",
    "                value=value+' '+req[i]\n",
    "            else:\n",
    "                value=value+req[i]\n",
    "    if value != '':\n",
    "        values.append(value)\n",
    "    test=dict(zip(key,values))\n",
    "    tests=[]\n",
    "    return test\n",
    "\n",
    "\n",
    "def doctor_reports(path):\n",
    "    global tests\n",
    "    '''Feed your pdf file'''\n",
    "    pdf = PdfFileReader(open(path,'rb'))\n",
    "\n",
    "    '''Store the content in a variable'''\n",
    "    content1=''\n",
    "    val=pdf.getNumPages()\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        content1 += pdf.getPage(i).extractText() + \"\\n\"\n",
    "\n",
    "    '''Split the content based on lines'''\n",
    "    req1=content1.strip().splitlines()\n",
    "    req1= [x.strip() for x in req1]\n",
    "    req1=list(filter(None,req1))\n",
    "\n",
    "    '''Algorithm to extract key value pairs'''\n",
    "    req_keys=['Patient Name','Age','Details','Symptoms','Tests','Lab report','Lab Id','Recommendation']\n",
    "    start=0\n",
    "    end=len(req_keys)\n",
    "    flag = False \n",
    "    final_keys=[]\n",
    "    temp_values=[]\n",
    "    final_values=[]\n",
    "    count = True\n",
    "    for i in req1:\n",
    "        if start < end:  \n",
    "            if req_keys[start] in i:\n",
    "                final_values.append(temp_values)\n",
    "                temp_values=[]\n",
    "                flag = True\n",
    "                final_keys.append(i)\n",
    "                start=start+1\n",
    "            elif flag:\n",
    "                temp_values.append(i)\n",
    "        else:\n",
    "            if req_keys[end-1] in i and f:\n",
    "                final_keys.append(i)\n",
    "                f = False\n",
    "            else:\n",
    "                temp_values.append(i)\n",
    "    final_values.append(temp_values)      \n",
    "    final_values.pop(0)\n",
    "    docFeatures=dict()\n",
    "    docFeaturesTemp=dict(zip(final_keys,final_values))\n",
    "    for k,v in docFeaturesTemp.items():\n",
    "        if not v:\n",
    "            items=k.split(':')\n",
    "            docFeatures[items[0].strip()]=items[1]\n",
    "        else:\n",
    "            k=k.replace(':','')\n",
    "            for z in v:\n",
    "                if ':' == z:\n",
    "                    v.remove(z)\n",
    "            docFeatures[k.strip()]=v\n",
    "\n",
    "\n",
    "\n",
    "    '''This is the final dictionary'''\n",
    "    docFeatures['Recommendation']=\"\".join(docFeatures['Recommendation'])\n",
    "    tests=docFeatures['Tests']\n",
    "    lab=docFeatures['Lab report'][0]\n",
    "    symptoms=docFeatures['Symptoms']\n",
    "    recommendation=docFeatures['Recommendation']\n",
    "    return lab,symptoms,recommendation\n",
    "\n",
    "\n",
    "def integration(doc_symptoms,doc_tests,recommendation):\n",
    "    features_and_values=OrderedDict()\n",
    "    feature_exists=[] \n",
    "    for i in doc_symptoms:\n",
    "        feature_exists=i.split(',')\n",
    "    for i in feature_s:\n",
    "        if i in feature_exists:\n",
    "            features_and_values[i]=\"Yes\"\n",
    "        else:\n",
    "            features_and_values[i]=\"No\"\n",
    "    if doc_tests != 'null':\n",
    "        for k,v in doc_tests.items():\n",
    "            features_and_values[k]=v\n",
    "    features_and_values['Recommendation']=recommendation\n",
    "    return features_and_values\n",
    "\n",
    "\n",
    "def createDataframeFromExtractedData():\n",
    "    dataframe=[]\n",
    "    keys=[]\n",
    "    temp=[]\n",
    "\n",
    "    for k,v in req_path.items():\n",
    "        lab,doc_symptoms,recommendation=doctor_reports(v[0])\n",
    "        doc_tests=\"null\"\n",
    "        if lab == \"Yes\":\n",
    "            doc_tests=lab_reports(v[1])\n",
    "        features_and_values=integration(doc_symptoms,doc_tests,recommendation)\n",
    "        dataframe.append(features_and_values)\n",
    "        \n",
    "\n",
    "    for d in dataframe:\n",
    "        if set(d.keys())-set(temp) :\n",
    "            temp=temp+list(OrderedDict.fromkeys(d))\n",
    "        \n",
    "        \n",
    "    temp=list(OrderedDict.fromkeys(temp))\n",
    "    df = pd.DataFrame(dataframe, columns=temp)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_path, content = extractZipFilesAndGetContent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = nltk.sent_tokenize(content) \n",
    "#sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_s = extractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame = createDataframeFromExtractedData()\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file=res.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "path='/home/dsxuser/work'+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_file(filename, filecontents):\n",
    "    '''Write file to Cloud Object Storage'''\n",
    "    resp = cos.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    return resp\n",
    "put_file(file_name, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
